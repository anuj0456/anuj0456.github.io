<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Realistic and consistent responses from LLMs by leveraging three senses | Anuj.dev</title>
<meta name="generator" content="Jekyll v4.3.4" />
<meta property="og:title" content="Realistic and consistent responses from LLMs by leveraging three senses" />
<meta name="author" content="Anuj Gupta" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="This blog is inspired from arXiv:2312.16233 Language models like GPT4, BARD, BedRock have been making quite a splash lately! From improving natural language understanding to aiding in various applications like chatbots, translation, and content generation, large language models (LLMs) have been at the forefront of AI advancements. The research and development in this field have led to more nuanced and context-aware language models, enabling better communication between humans and machines. The realism and consistency of these responses can be further enhanced by providing richer information of the agent being mimicked. But due to the significant computational resources required for training such models, prompt tuning has emerged as a crucial aspect of optimizing LLM performance. Recent research has explored various techniques to generate more realistic responses through effective prompt engineering, such as prompting a relevant pseudo dialogue or providing detailed information of the scene, relations, and attributes.The innate context limit of LLMs poses a challenge for maintaining a consistent conversational memory. To address these challenges, a multi-pronged approach aimed at enhancing the efficacy of prompts for LLMs can be used: Information-rich Prompting — Initialize and continuously update the prompts so that it provides multi-aspect information on the character.Within-prompt Self Memory Management — To mitigate the limitation of context length, make the language model to summarize the history log and maintain it in the prompt.Benchmark Dataset — To overcome the scarcity of useful datasets for evaluation, augment Cornell MovieDialog Corpus2 via GPT-3.5 Turbo, a model known for its strong capabilities comparable to those of fine-tuned LLMs Results Each component in the approach helps generate a better utterance. In conclusion, the study highlights the effectiveness of information-rich prompting in enhancing the naturalness and realism of utterance generation when the language model emulates a fictional character. REFERENCE: arXiv:2312.16233" />
<meta property="og:description" content="This blog is inspired from arXiv:2312.16233 Language models like GPT4, BARD, BedRock have been making quite a splash lately! From improving natural language understanding to aiding in various applications like chatbots, translation, and content generation, large language models (LLMs) have been at the forefront of AI advancements. The research and development in this field have led to more nuanced and context-aware language models, enabling better communication between humans and machines. The realism and consistency of these responses can be further enhanced by providing richer information of the agent being mimicked. But due to the significant computational resources required for training such models, prompt tuning has emerged as a crucial aspect of optimizing LLM performance. Recent research has explored various techniques to generate more realistic responses through effective prompt engineering, such as prompting a relevant pseudo dialogue or providing detailed information of the scene, relations, and attributes.The innate context limit of LLMs poses a challenge for maintaining a consistent conversational memory. To address these challenges, a multi-pronged approach aimed at enhancing the efficacy of prompts for LLMs can be used: Information-rich Prompting — Initialize and continuously update the prompts so that it provides multi-aspect information on the character.Within-prompt Self Memory Management — To mitigate the limitation of context length, make the language model to summarize the history log and maintain it in the prompt.Benchmark Dataset — To overcome the scarcity of useful datasets for evaluation, augment Cornell MovieDialog Corpus2 via GPT-3.5 Turbo, a model known for its strong capabilities comparable to those of fine-tuned LLMs Results Each component in the approach helps generate a better utterance. In conclusion, the study highlights the effectiveness of information-rich prompting in enhancing the naturalness and realism of utterance generation when the language model emulates a fictional character. REFERENCE: arXiv:2312.16233" />
<link rel="canonical" href="https://medium.com/@anuj0456/realistic-and-consistent-responses-from-llms-by-leveraging-five-senses-01e9d46f3c9c?source=rss-d2453bdab35d------2" />
<meta property="og:url" content="https://medium.com/@anuj0456/realistic-and-consistent-responses-from-llms-by-leveraging-five-senses-01e9d46f3c9c?source=rss-d2453bdab35d------2" />
<meta property="og:site_name" content="Anuj.dev" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-04-24T00:00:00+05:30" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Realistic and consistent responses from LLMs by leveraging three senses" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Anuj Gupta"},"dateModified":"2024-04-24T00:00:00+05:30","datePublished":"2024-04-24T00:00:00+05:30","description":"This blog is inspired from arXiv:2312.16233 Language models like GPT4, BARD, BedRock have been making quite a splash lately! From improving natural language understanding to aiding in various applications like chatbots, translation, and content generation, large language models (LLMs) have been at the forefront of AI advancements. The research and development in this field have led to more nuanced and context-aware language models, enabling better communication between humans and machines. The realism and consistency of these responses can be further enhanced by providing richer information of the agent being mimicked. But due to the significant computational resources required for training such models, prompt tuning has emerged as a crucial aspect of optimizing LLM performance. Recent research has explored various techniques to generate more realistic responses through effective prompt engineering, such as prompting a relevant pseudo dialogue or providing detailed information of the scene, relations, and attributes.The innate context limit of LLMs poses a challenge for maintaining a consistent conversational memory. To address these challenges, a multi-pronged approach aimed at enhancing the efficacy of prompts for LLMs can be used: Information-rich Prompting — Initialize and continuously update the prompts so that it provides multi-aspect information on the character.Within-prompt Self Memory Management — To mitigate the limitation of context length, make the language model to summarize the history log and maintain it in the prompt.Benchmark Dataset — To overcome the scarcity of useful datasets for evaluation, augment Cornell MovieDialog Corpus2 via GPT-3.5 Turbo, a model known for its strong capabilities comparable to those of fine-tuned LLMs Results Each component in the approach helps generate a better utterance. In conclusion, the study highlights the effectiveness of information-rich prompting in enhancing the naturalness and realism of utterance generation when the language model emulates a fictional character. REFERENCE: arXiv:2312.16233","headline":"Realistic and consistent responses from LLMs by leveraging three senses","mainEntityOfPage":{"@type":"WebPage","@id":"https://medium.com/@anuj0456/realistic-and-consistent-responses-from-llms-by-leveraging-five-senses-01e9d46f3c9c?source=rss-d2453bdab35d------2"},"url":"https://medium.com/@anuj0456/realistic-and-consistent-responses-from-llms-by-leveraging-five-senses-01e9d46f3c9c?source=rss-d2453bdab35d------2"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Anuj.dev" />
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Anuj.dev</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a><a class="page-link" href="/pilottai/">PilottAI</a><a class="page-link" href="/ailert/">AiLert</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper"><article
  class="post h-entry"
  itemscope
  itemtype="http://schema.org/BlogPosting"
>
  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">
      Realistic and consistent responses from LLMs by leveraging three senses
    </h1>
    <p class="post-meta"><time
        class="dt-published"
        datetime="2024-04-24T00:00:00+05:30"
        itemprop="datePublished"
      >
        Apr-24-2024
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody"><figure><img alt="" src="https://cdn-images-1.medium.com/max/640/0*psnZslMufl6xVVd0.jpg" /></figure><blockquote>This blog is inspired from <a href="https://arxiv.org/abs/2312.16233">arXiv:2312.16233</a></blockquote><p>Language models like GPT4, BARD, BedRock have been making quite a splash lately! From improving natural language understanding to aiding in various applications like chatbots, translation, and content generation, large language models (LLMs) have been at the forefront of AI advancements. The research and development in this field have led to more nuanced and context-aware language models, enabling better communication between humans and machines. The realism and consistency of these responses can be further enhanced by providing richer information of the agent being mimicked.</p><p>But due to the significant computational resources required for training such models, prompt tuning has emerged as a crucial aspect of optimizing LLM performance. Recent research has explored various techniques to generate more realistic responses through effective prompt engineering, such as prompting a relevant pseudo dialogue or providing detailed information of the scene, relations, and attributes.The innate context limit of LLMs poses a challenge for maintaining a consistent conversational memory.</p><p>To address these challenges, a multi-pronged approach aimed at enhancing the efficacy of prompts for LLMs can be used:</p><ol><li>Information-rich Prompting — Initialize and continuously update the prompts so that it provides multi-aspect information on the character.</li><li>Within-prompt Self Memory Management — To mitigate the limitation of context length, make the language model to summarize the history log and maintain it in the prompt.</li><li>Benchmark Dataset — To overcome the scarcity of useful datasets for evaluation, augment Cornell MovieDialog Corpus2 via GPT-3.5 Turbo, a model known for its strong capabilities comparable to those of fine-tuned LLMs</li></ol><p>Results</p><p>Each component in the approach helps generate a better utterance.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/670/1*91tv-GY-vXnMs-mzzWvaSw.png" /></figure><p>In conclusion, the study highlights the effectiveness of information-rich prompting in enhancing the naturalness and realism of utterance generation when the language model emulates a fictional character.</p><p><strong>REFERENCE: </strong><a href="https://arxiv.org/abs/2312.16233"><strong>arXiv:2312.16233</strong></a></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=01e9d46f3c9c" width="1" height="1" alt="">
</div><a class="u-url" href="/2024/04/24/realistic-and-consistent-responses-from-llms-by-leveraging-three-senses.html" hidden></a>
</article>
</div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="http://localhost:4000/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
        <ul class="contact-list">
          <li class="p-name">Anuj Gupta</li>
          <li><a class="u-email" href="mailto:anuj0456@gmail.com">anuj0456@gmail.com</a></li>
        </ul>
      </div>
      <div class="footer-col">
        <p>Welcome to Anuj&#39;s digital space! Explore my journey in AI,  tech, and entrepreneurship. Discover projects, insights,  and innovations that shape the future.
</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li>
  <a rel="me" href="https://dev.to/anuj0456" target="_blank" title="devto">
    <svg class="svg-icon grey">
      <use xlink:href="/assets/minima-social-icons.svg#devto"></use>
    </svg>
  </a>
</li>
<li>
  <a rel="me" href="https://github.com/anuj0456" target="_blank" title="github">
    <svg class="svg-icon grey">
      <use xlink:href="/assets/minima-social-icons.svg#github"></use>
    </svg>
  </a>
</li>
<li>
  <a rel="me" href="https://www.linkedin.com/in/anuj-gupta-9a691527/" target="_blank" title="linkedin">
    <svg class="svg-icon grey">
      <use xlink:href="/assets/minima-social-icons.svg#linkedin"></use>
    </svg>
  </a>
</li>
<li>
  <a rel="me" href="https://stackoverflow.com/users/3569546/anuj0456" target="_blank" title="stackoverflow">
    <svg class="svg-icon grey">
      <use xlink:href="/assets/minima-social-icons.svg#stackoverflow"></use>
    </svg>
  </a>
</li>
<li>
  <a rel="me" href="https://x.com/anuj0456" target="_blank" title="twitter">
    <svg class="svg-icon grey">
      <use xlink:href="/assets/minima-social-icons.svg#twitter"></use>
    </svg>
  </a>
</li>
</ul>
</div>

  </div>

</footer>
</body>
</html>
